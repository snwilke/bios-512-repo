{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d684172-4b94-42cf-bda2-e11952420d86",
   "metadata": {},
   "source": [
    "# Homework 10\n",
    "#### Course Notes\n",
    "**Language Models:** https://github.com/rjenki/BIOS512/tree/main/lecture17  \n",
    "**Unix:** https://github.com/rjenki/BIOS512/tree/main/lecture18  \n",
    "**Docker:** https://github.com/rjenki/BIOS512/tree/main/lecture19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d839a5ba-62f4-4699-baea-018afda70786",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "#### Make a language model that uses ngrams and allows the user to specify start words, but uses a random start if one is not specified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef37d3a-a6ad-42ae-9e16-7d7338c9ce49",
   "metadata": {},
   "source": [
    "#### a) Make a function to tokenize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c519d3fa-1ff2-45e6-80c8-fe51f008223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize <- function(text) {\n",
    "  text <- tolower(text)\n",
    "  unlist(regmatches(text, gregexpr(\"[a-z']+|[.?!]\", text, perl = TRUE)))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86145513-294b-4894-a02c-8ae60e2c616e",
   "metadata": {},
   "source": [
    "#### b) Make a function generate keys for ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63d97df5-366a-486f-8a4f-560cc23ba5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    ".join_key  <- function(x) paste(x, collapse = \"\\x1F\")\n",
    ".split_key <- function(k) strsplit(k, \"\\x1F\", fixed = TRUE)[[1]]\n",
    "\n",
    "ngram_pairs <- function(tokens, n = 3) {\n",
    "  if (n < 2) stop(\"n must be >= 2\")\n",
    "  if (length(tokens) < n)\n",
    "    return(data.frame(key = character(0), next_token = character(0), stringsAsFactors = FALSE))\n",
    "  idx  <- seq_len(length(tokens) - n + 1)\n",
    "  keys <- vapply(idx, function(i) .join_key(tokens[i:(i + n - 2)]), character(1))\n",
    "  nexts <- tokens[n:length(tokens)]\n",
    "  data.frame(key = keys, next_token = nexts, stringsAsFactors = FALSE)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52988c2c-b230-467f-b519-72bc85b93b43",
   "metadata": {},
   "source": [
    "#### c) Make a function to build an ngram table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47646e75-f17e-4d79-9a41-cfb3eaa035c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_ngram_table <- function(pairs) {\n",
    "  if (!nrow(pairs)) return(list())\n",
    "  split(pairs$next_token, pairs$key)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca6db37-abce-4705-9784-e1b898174f00",
   "metadata": {},
   "source": [
    "#### d) Function to digest the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e7abfb9-96c7-44d6-b53d-2c138e46b738",
   "metadata": {},
   "outputs": [],
   "source": [
    "digest_text <- function(text, n = 3) {\n",
    "  toks  <- tokenize(text)\n",
    "  pairs <- ngram_pairs(toks, n)\n",
    "  table <- build_ngram_table(pairs)\n",
    "  list(table = table, n = n)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fff313-0f13-479b-94df-7588c19fdd3d",
   "metadata": {},
   "source": [
    "#### e) Function to digest the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e95b6dbe-6856-4290-b71e-c3fc5b4e59f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "digest_url <- function(u, n = 3) {\n",
    "  text <- tryCatch({\n",
    "    readr::read_file(u)\n",
    "  }, error = function(e) {\n",
    "    con <- url(u, open = \"rb\")\n",
    "    on.exit(try(close(con), silent = TRUE), add = TRUE)\n",
    "    paste(readLines(con, warn = FALSE, encoding = \"UTF-8\"), collapse = \"\\n\")\n",
    "  })\n",
    "  text <- gsub(\"<[^>]+>\", \" \", text)\n",
    "  digest_text(text, n)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa4e73-ee6f-4569-9a54-9d7f7eb3f80a",
   "metadata": {},
   "source": [
    "#### f) Function that gives random start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6654612-8e26-4f7e-bdd4-0ada2bb699d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_start <- function(table) {\n",
    "  key <- sample(names(table), 1)\n",
    "  .split_key(key)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e998fb24-f2d6-41bc-a751-1f6accd3411f",
   "metadata": {},
   "source": [
    "#### g) Function to predict the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1d0a148-a7b0-42cb-881f-e89a09ea20bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_next_word <- function(table, history, n) {\n",
    "  if (length(history) < (n - 1)) return(NA_character_)\n",
    "  key <- .join_key(tail(history, n - 1))\n",
    "  if (!key %in% names(table)) return(NA_character_)\n",
    "  sample(table[[key]], 1)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347f4002-4932-42c4-a4af-8689293a5857",
   "metadata": {},
   "source": [
    "#### h) Function that puts everything together. Specify that if the user does not give a start word, then the random start will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ae04ca3-2cea-47e4-918e-d11ebe8aed74",
   "metadata": {},
   "outputs": [],
   "source": [
    ".detokenize <- function(tokens) {\n",
    "  if (!length(tokens)) return(\"\")\n",
    "  out <- tokens[1]\n",
    "  if (length(tokens) > 1) {\n",
    "    for (tok in tokens[-1]) {\n",
    "      if (tok %in% c(\".\", \"?\", \"!\")) out <- paste0(out, tok) else out <- paste(out, tok)\n",
    "    }\n",
    "  }\n",
    "  out\n",
    "}\n",
    "\n",
    "generate_text <- function(table, n, max_words = 50, start_words = NULL) {\n",
    "  if (is.null(start_words) || length(start_words) < (n - 1) ||\n",
    "      !.join_key(tail(start_words, n - 1)) %in% names(table)) {\n",
    "    history <- random_start(table)\n",
    "  } else {\n",
    "    history <- tail(start_words, n - 1)\n",
    "  }\n",
    "  generated <- history\n",
    "  for (i in seq_len(max_words)) {\n",
    "    nxt <- predict_next_word(table, history, n)\n",
    "    if (is.na(nxt)) break\n",
    "    generated <- c(generated, nxt)\n",
    "    history <- tail(generated, n - 1)\n",
    "  }\n",
    "  .detokenize(generated)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b742c67-907c-4bc7-8df1-c84fa65a7554",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "#### For this question, set `seed=2025`.\n",
    "#### a) Test your model using a text file of [Grimm's Fairy Tails](https://www.gutenberg.org/cache/epub/2591/pg2591.txt)\n",
    "#### i) Using n=3, with the start word(s) \"the king\", with length=15. \n",
    "#### ii) Using n=3, with no start word, with length=15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ffef8a6-42dd-4460-a144-13e2df112c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.i:\n",
      "the king said to the king and accused him of his friends and both studied law at \n",
      "\n",
      "a.ii:\n",
      "horsemen would not rest until the liquid ran out of the wine ran out. then he \n",
      "\n"
     ]
    }
   ],
   "source": [
    "set.seed(2025)\n",
    "n  <- 3L\n",
    "L  <- 15L  # number of tokens to generate\n",
    "u_grimm  <- \"https://www.gutenberg.org/cache/epub/2591/pg2591.txt\"\n",
    "u_armour <- \"https://www.gutenberg.org/cache/epub/46342/pg46342.txt\"\n",
    "\n",
    "m_grimm  <- digest_url(u_grimm,  n = n)\n",
    "m_armour <- digest_url(u_armour, n = n)\n",
    "# i) n=3, start = \"the king\", length = 15\n",
    "cat(\"a.i:\\n\")\n",
    "out_ai  <- generate_text(m_grimm$table, m_grimm$n, max_words = L, start_words = c(\"the\",\"king\"))\n",
    "cat(out_ai, \"\\n\\n\")\n",
    "\n",
    "# ii) n=3, no start word, length = 15\n",
    "cat(\"a.ii:\\n\")\n",
    "out_aii <- generate_text(m_grimm$table, m_grimm$n, max_words = L)\n",
    "cat(out_aii, \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e04b167-7f2c-4e0f-88e7-86ba5e8d74cc",
   "metadata": {},
   "source": [
    "#### b) Test your model using a text file of [Ancient Armour and Weapons in Europe](https://www.gutenberg.org/cache/epub/46342/pg46342.txt)\n",
    "#### i) Using n=3, with the start word(s) \"the king\", with length=15. \n",
    "#### ii) Using n=3, with no start word, with length=15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4746e2a-adf7-45c1-8765-9e05f24746b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b.i:\n",
      "the king of england have been found with the description left us by sidonius as forming part \n",
      "\n",
      "b.ii:\n",
      "this incident gives it in the pourpointed chausson worked in the last of the weapons disclosed by \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# i) n=3, start = \"the king\", length = 15\n",
    "cat(\"b.i:\\n\")\n",
    "out_bi  <- generate_text(m_armour$table, m_armour$n, max_words = L, start_words = c(\"the\",\"king\"))\n",
    "cat(out_bi, \"\\n\\n\")\n",
    "\n",
    "# ii) n=3, no start word, length = 15\n",
    "cat(\"b.ii:\\n\")\n",
    "out_bii <- generate_text(m_armour$table, m_armour$n, max_words = L)\n",
    "cat(out_bii, \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fb37ad-8e7c-4e62-afc0-ba46d46401fc",
   "metadata": {},
   "source": [
    "#### c) Explain in 1-2 sentences the difference in content generated from each source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97a62e8-e997-45cc-87e3-a72f30a0a4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grimm’s text produces narrative, character‑driven phrases (story events, dialogue, motifs), whereas the armour text yields technical, expository language about materials, weapons, and historical context; starting from “the king” fits Grimm naturally but in the armour corpus it is rarer and tends to drift quickly into descriptive prose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e45972-f441-4d07-9073-fcddd6146cbd",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "#### a) What is a language learning model? \n",
    "#### b) Imagine the internet goes down and you can't run to your favorite language model for help. How do you run one locally?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3dde26-02c5-4f2c-a63d-e418070fa7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) A model that assigns probabilities to token sequences and predicts the next token from context; modern LLMs are neural (usually Transformers) trained via next‑token prediction on large corpora.\n",
    "# B) Install OLLAMA, check the version with `ollama -v, Pull or install a model locally:** `ollama pull gemma3:1b`.\n",
    "4.  **Run the OLLAMA API server:** `ollama serve`. If it says it's already in use, check `lsof -i :11434`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85a743b-f814-4a53-96e6-8bccb3d34ab8",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "#### Explain what the following vocab words mean in the context of typing `mkdir project` into the command line. If the term doesn't apply to this command, give the definition and/or an example.\n",
    "| Term | Meaning |  \n",
    "|------|---------|\n",
    "| **Shell** |  |\n",
    "| **Terminal emulator** |  |\n",
    "| **Process** |  |\n",
    "| **Signal** |  |\n",
    "| **Standard input** |  |\n",
    "| **Standard output** |  |\n",
    "| **Command line argument** |  |\n",
    "| **The environment** |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1332ff27-ca3f-4f7e-b4b9-07ead0358dd2",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "#### Consider the following command `find . -iname \"*.R\" | xargs grep read_csv`.\n",
    "#### a) What are the programs?\n",
    "#### b) Explain what this command is doing, part by part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69771ac7-865e-4d82-aa25-a39e7c1ab095",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "#### Install Docker on your machine. See [here](https://github.com/rjenki/BIOS512/blob/main/lecture18/docker_install.md) for instructions. \n",
    "#### a) Show the response when you run `docker run hello-world`.\n",
    "#### b) Access Rstudio through a Docker container. Set your password and make sure your files show up on the Rstudio server. Type the command and the output you get below.\n",
    "#### c) How do you log in to the RStudio server?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
